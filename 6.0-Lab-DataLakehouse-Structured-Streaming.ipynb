{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f6cce12-a006-4a6f-aa2d-49ffb1836a2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lab 06: Data Lakehouse with Structured Streaming\n",
    "This lab will help you learn to use many of the software libraries and programming techniques required to fulfill the requirements of the final end-of-session capstone project for course **DS-2002: Data Systems**. The spirit of the project is to provide a capstone challenge that requires students to demonstrate a practical and functional understanding of each of the data systems and architectural principles covered throughout the session.\n",
    "\n",
    "**These include:**\n",
    "- Relational Database Management Systems (e.g., MySQL, Microsoft SQL Server, Oracle, IBM DB2)\n",
    "  - Online Transaction Processing Systems (OLTP): *Optimized for High-Volume Write Operations; Normalized to 3rd Normal Form.*\n",
    "  - Online Analytical Processing Systems (OLAP): *Optimized for Read/Aggregation Operations; Dimensional Model (i.e, Star Schema)*\n",
    "- NoSQL *(Not Only SQL)* Systems (e.g., MongoDB, CosmosDB, Cassandra, HBase, Redis)\n",
    "- File System *(Data Lake)* Source Systems (e.g., AWS S3, Microsoft Azure Data Lake Storage)\n",
    "  - Various Datafile Formats (e.g., JSON, CSV, Parquet, Text, Binary)\n",
    "- Massively Parallel Processing *(MPP)* Data Integration Systems (e.g., Apache Spark, Databricks)\n",
    "- Data Integration Patterns (e.g., Extract-Transform-Load, Extract-Load-Transform, Extract-Load-Transform-Load, Lambda & Kappa Architectures)\n",
    "\n",
    "### Section I: Prerequisites\n",
    "\n",
    "#### 1.0. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6a8545c-e396-40dd-a0eb-8fb98663bef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n  Obtaining dependency information for pymongo from https://files.pythonhosted.org/packages/9a/16/dbffca9d4ad66f2a325c280f1177912fa23235987f7b9033e283da889b7a/pymongo-4.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pymongo-4.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\nCollecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n  Obtaining dependency information for dnspython<3.0.0,>=1.16.0 from https://files.pythonhosted.org/packages/68/1b/e0a87d256e40e8c888847551b20a017a6b98139178505dc7ffb96f04e954/dnspython-2.7.0-py3-none-any.whl.metadata\n  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\nDownloading pymongo-4.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.3/1.7 MB\u001B[0m \u001B[31m11.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m1.6/1.7 MB\u001B[0m \u001B[31m29.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m23.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/313.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m313.6/313.6 kB\u001B[0m \u001B[31m27.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: dnspython, pymongo\nSuccessfully installed dnspython-2.7.0 pymongo-4.10.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "%pip install pymongo\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pymongo\n",
    "import pyspark.pandas as pd  # This uses Koalas that is included in PySpark version 3.2 or newer.\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, BinaryType\n",
    "from pyspark.sql.types import ByteType, ShortType, IntegerType, LongType, FloatType, DecimalType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c43fc7d-2918-4270-8595-354881a4e36d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.0. Instantiate Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae18ed4a-a051-4249-ac4d-55d27a42b4e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Azure MySQL Server Connection Information ###################\n",
    "jdbc_hostname = \"wna8fw-mysql.mysql.database.azure.com\"\n",
    "jdbc_port = 3306\n",
    "src_database = \"northwind_dw2\"\n",
    "\n",
    "connection_properties = {\n",
    "  \"user\" : \"root\",\n",
    "  \"password\" : \"Luckylemons12$\",\n",
    "  \"driver\" : \"org.mariadb.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# MongoDB Atlas Connection Information ########################\n",
    "atlas_cluster_name = \"Cluster0\"\n",
    "atlas_database_name = \"northwind_dw2\"\n",
    "atlas_user_name = \"jsq4xr\"\n",
    "atlas_password = \"5DltCXGlClkT1XtZ\"\n",
    "\n",
    "# Data Files (JSON) Information ###############################\n",
    "dst_database = \"northwind_dlh\"\n",
    "\n",
    "base_dir = \"dbfs:/FileStore/lab_data\"\n",
    "database_dir = f\"{base_dir}/{dst_database}\"\n",
    "\n",
    "data_dir = f\"{base_dir}/retail\"\n",
    "batch_dir = f\"{data_dir}/batch\"\n",
    "stream_dir = f\"{data_dir}/stream\"\n",
    "\n",
    "orders_stream_dir = f\"{stream_dir}/orders\"\n",
    "purchase_orders_stream_dir = f\"{stream_dir}/purchase_orders\"\n",
    "inventory_trans_stream_dir = f\"{stream_dir}/inventory_transactions\"\n",
    "\n",
    "orders_output_bronze = f\"{database_dir}/fact_orders/bronze\"\n",
    "orders_output_silver = f\"{database_dir}/fact_orders/silver\"\n",
    "orders_output_gold   = f\"{database_dir}/fact_orders/gold\"\n",
    "\n",
    "purchase_orders_output_bronze = f\"{database_dir}/fact_purchase_orders/bronze\"\n",
    "purchase_orders_output_silver = f\"{database_dir}/fact_purchase_orders/silver\"\n",
    "purchase_orders_output_gold   = f\"{database_dir}/fact_purchase_orders/gold\"\n",
    "\n",
    "inventory_trans_output_bronze = f\"{database_dir}/fact_inventory_transactions/bronze\"\n",
    "inventory_trans_output_silver = f\"{database_dir}/fact_inventory_transactions/silver\"\n",
    "inventory_trans_output_gold   = f\"{database_dir}/fact_inventory_transactions/gold\"\n",
    "\n",
    "# Delete the Streaming Files ################################## \n",
    "dbutils.fs.rm(f\"{database_dir}/fact_orders\", True) \n",
    "dbutils.fs.rm(f\"{database_dir}/fact_purchase_orders\", True) \n",
    "dbutils.fs.rm(f\"{database_dir}/fact_inventory_transactions\", True)\n",
    "\n",
    "# Delete the Database Files ###################################\n",
    "dbutils.fs.rm(database_dir, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e33d70f-637e-4eb6-a1a8-c00ea26011a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3.0. Define Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "939af89f-638e-44fd-8e58-f88591889f4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##################################################################################################################\n",
    "# Use this Function to Fetch a DataFrame from the MongoDB Atlas database server Using PyMongo.\n",
    "##################################################################################################################\n",
    "def get_mongo_dataframe(user_id, pwd, cluster_name, db_name, collection, conditions, projection, sort):\n",
    "    '''Create a client connection to MongoDB'''\n",
    "    mongo_uri = f\"mongodb+srv://{user_id}:{pwd}@{cluster_name}.mongodb.net/{db_name}\"\n",
    "    \n",
    "    client = pymongo.MongoClient(mongo_uri)\n",
    "\n",
    "    '''Query MongoDB, and fill a python list with documents to create a DataFrame'''\n",
    "    db = client[db_name]\n",
    "    if conditions and projection and sort:\n",
    "        dframe = pd.DataFrame(list(db[collection].find(conditions, projection).sort(sort)))\n",
    "    elif conditions and projection and not sort:\n",
    "        dframe = pd.DataFrame(list(db[collection].find(conditions, projection)))\n",
    "    else:\n",
    "        dframe = pd.DataFrame(list(db[collection].find()))\n",
    "\n",
    "    client.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "##################################################################################################################\n",
    "# Use this Function to Create New Collections by Uploading JSON file(s) to the MongoDB Atlas server.\n",
    "##################################################################################################################\n",
    "def set_mongo_collection(user_id, pwd, cluster_name, db_name, src_file_path, json_files):\n",
    "    '''Create a client connection to MongoDB'''\n",
    "    mongo_uri = \"mongodb+srv://jsq4xr:5DltCXGlClkT1XtZ@cluster0.laoqh.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "    client = pymongo.MongoClient(mongo_uri)\n",
    "    db = client[db_name]\n",
    "    \n",
    "    '''Read in a JSON file, and Use It to Create a New Collection'''\n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(src_file_path, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "\n",
    "    client.close()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8066e8d6-3626-449d-a985-2fce976d0914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Section II: Populate Dimensions by Ingesting Reference (Cold-path) Data \n",
    "#### 1.0. Fetch Reference Data From an Azure MySQL Database\n",
    "##### 1.1. Create a New Databricks Metadata Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd70267e-3bc4-4f48-a7ef-4ea4d2dcc3c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DROP DATABASE IF EXISTS northwind_dlh CASCADE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8753d7a5-d652-4163-b69f-71355180c93a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS northwind_dlh\n",
    "COMMENT \"DS-2002 Lab 06 Database\"\n",
    "LOCATION \"dbfs:/FileStore/lab_data/northwind_dlh\"\n",
    "WITH DBPROPERTIES (contains_pii = true, purpose = \"DS-2002 Lab 6.0\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b8250c5-9f2b-4b9b-9e3a-6e5fcd2b3aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 1.2. Create a New Table that Sources Date Dimension Data from a Table in an Azure MySQL database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc64a21-eb30-4d1d-9ed9-9258593d5281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW view_date\n",
    "USING org.apache.spark.sql.jdbc\n",
    "OPTIONS (\n",
    "  url \"jdbc:mysql://wna8fw-mysql.mysql.database.azure.com:3306/northwind_dw2\", --Replace with your Server Name\n",
    "  dbtable \"dim_date\",\n",
    "  user \"jtupitza\",    --Replace with your User Name\n",
    "  password \"Passw0rd123\"  --Replace with you password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "015675a3-1b74-4f42-b00b-df8b9f2cfa0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 45
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE DATABASE northwind_dlh;\n",
    "\n",
    "CREATE OR REPLACE TABLE northwind_dlh.dim_date\n",
    "COMMENT \"Date Dimension Table\"\n",
    "LOCATION \"dbfs:/FileStore/lab_data/northwind_dlh/dim_date\"\n",
    "AS SELECT * FROM view_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "368e6706-b13d-4be4-98dd-16956e85d40b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>date_key</td><td>int</td><td>null</td></tr><tr><td>full_date</td><td>date</td><td>null</td></tr><tr><td>date_name</td><td>varchar(11)</td><td>null</td></tr><tr><td>date_name_us</td><td>varchar(11)</td><td>null</td></tr><tr><td>date_name_eu</td><td>varchar(11)</td><td>null</td></tr><tr><td>day_of_week</td><td>tinyint</td><td>null</td></tr><tr><td>day_name_of_week</td><td>varchar(10)</td><td>null</td></tr><tr><td>day_of_month</td><td>tinyint</td><td>null</td></tr><tr><td>day_of_year</td><td>int</td><td>null</td></tr><tr><td>weekday_weekend</td><td>varchar(10)</td><td>null</td></tr><tr><td>week_of_year</td><td>tinyint</td><td>null</td></tr><tr><td>month_name</td><td>varchar(10)</td><td>null</td></tr><tr><td>month_of_year</td><td>tinyint</td><td>null</td></tr><tr><td>is_last_day_of_month</td><td>varchar(1)</td><td>null</td></tr><tr><td>calendar_quarter</td><td>tinyint</td><td>null</td></tr><tr><td>calendar_year</td><td>int</td><td>null</td></tr><tr><td>calendar_year_month</td><td>varchar(10)</td><td>null</td></tr><tr><td>calendar_year_qtr</td><td>varchar(10)</td><td>null</td></tr><tr><td>fiscal_month_of_year</td><td>tinyint</td><td>null</td></tr><tr><td>fiscal_quarter</td><td>tinyint</td><td>null</td></tr><tr><td>fiscal_year</td><td>int</td><td>null</td></tr><tr><td>fiscal_year_month</td><td>varchar(10)</td><td>null</td></tr><tr><td>fiscal_year_qtr</td><td>varchar(10)</td><td>null</td></tr><tr><td></td><td></td><td></td></tr><tr><td># Delta Statistics Columns</td><td></td><td></td></tr><tr><td>Column Names</td><td>month_of_year, weekday_weekend, date_name_us, day_of_week, date_key, calendar_quarter, calendar_year_month, date_name_eu, date_name, fiscal_quarter, is_last_day_of_month, day_of_month, month_name, calendar_year_qtr, fiscal_year_qtr, full_date, fiscal_year_month, fiscal_month_of_year, calendar_year, day_name_of_week, day_of_year, week_of_year, fiscal_year</td><td></td></tr><tr><td>Column Selection Method</td><td>first-32</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td># Detailed Table Information</td><td></td><td></td></tr><tr><td>Catalog</td><td>hive_metastore</td><td></td></tr><tr><td>Database</td><td>northwind_dlh</td><td></td></tr><tr><td>Table</td><td>dim_date</td><td></td></tr><tr><td>Created Time</td><td>Wed Nov 27 20:48:22 UTC 2024</td><td></td></tr><tr><td>Last Access</td><td>UNKNOWN</td><td></td></tr><tr><td>Created By</td><td>Spark 3.5.0</td><td></td></tr><tr><td>Type</td><td>EXTERNAL</td><td></td></tr><tr><td>Comment</td><td>Date Dimension Table</td><td></td></tr><tr><td>Location</td><td>dbfs:/FileStore/lab_data/northwind_dlh/dim_date</td><td></td></tr><tr><td>Provider</td><td>delta</td><td></td></tr><tr><td>Owner</td><td>root</td><td></td></tr><tr><td>Table Properties</td><td>[delta.enableDeletionVectors=true,delta.feature.deletionVectors=supported,delta.minReaderVersion=3,delta.minWriterVersion=7]</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "date_key",
         "int",
         null
        ],
        [
         "full_date",
         "date",
         null
        ],
        [
         "date_name",
         "varchar(11)",
         null
        ],
        [
         "date_name_us",
         "varchar(11)",
         null
        ],
        [
         "date_name_eu",
         "varchar(11)",
         null
        ],
        [
         "day_of_week",
         "tinyint",
         null
        ],
        [
         "day_name_of_week",
         "varchar(10)",
         null
        ],
        [
         "day_of_month",
         "tinyint",
         null
        ],
        [
         "day_of_year",
         "int",
         null
        ],
        [
         "weekday_weekend",
         "varchar(10)",
         null
        ],
        [
         "week_of_year",
         "tinyint",
         null
        ],
        [
         "month_name",
         "varchar(10)",
         null
        ],
        [
         "month_of_year",
         "tinyint",
         null
        ],
        [
         "is_last_day_of_month",
         "varchar(1)",
         null
        ],
        [
         "calendar_quarter",
         "tinyint",
         null
        ],
        [
         "calendar_year",
         "int",
         null
        ],
        [
         "calendar_year_month",
         "varchar(10)",
         null
        ],
        [
         "calendar_year_qtr",
         "varchar(10)",
         null
        ],
        [
         "fiscal_month_of_year",
         "tinyint",
         null
        ],
        [
         "fiscal_quarter",
         "tinyint",
         null
        ],
        [
         "fiscal_year",
         "int",
         null
        ],
        [
         "fiscal_year_month",
         "varchar(10)",
         null
        ],
        [
         "fiscal_year_qtr",
         "varchar(10)",
         null
        ],
        [
         "",
         "",
         ""
        ],
        [
         "# Delta Statistics Columns",
         "",
         ""
        ],
        [
         "Column Names",
         "month_of_year, weekday_weekend, date_name_us, day_of_week, date_key, calendar_quarter, calendar_year_month, date_name_eu, date_name, fiscal_quarter, is_last_day_of_month, day_of_month, month_name, calendar_year_qtr, fiscal_year_qtr, full_date, fiscal_year_month, fiscal_month_of_year, calendar_year, day_name_of_week, day_of_year, week_of_year, fiscal_year",
         ""
        ],
        [
         "Column Selection Method",
         "first-32",
         ""
        ],
        [
         "",
         "",
         ""
        ],
        [
         "# Detailed Table Information",
         "",
         ""
        ],
        [
         "Catalog",
         "hive_metastore",
         ""
        ],
        [
         "Database",
         "northwind_dlh",
         ""
        ],
        [
         "Table",
         "dim_date",
         ""
        ],
        [
         "Created Time",
         "Wed Nov 27 20:48:22 UTC 2024",
         ""
        ],
        [
         "Last Access",
         "UNKNOWN",
         ""
        ],
        [
         "Created By",
         "Spark 3.5.0",
         ""
        ],
        [
         "Type",
         "EXTERNAL",
         ""
        ],
        [
         "Comment",
         "Date Dimension Table",
         ""
        ],
        [
         "Location",
         "dbfs:/FileStore/lab_data/northwind_dlh/dim_date",
         ""
        ],
        [
         "Provider",
         "delta",
         ""
        ],
        [
         "Owner",
         "root",
         ""
        ],
        [
         "Table Properties",
         "[delta.enableDeletionVectors=true,delta.feature.deletionVectors=supported,delta.minReaderVersion=3,delta.minWriterVersion=7]",
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 47
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\":\"name of the column\"}",
         "name": "col_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"data type of the column\"}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"comment of the column\"}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED northwind_dlh.dim_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f395402-1acd-49e7-85e7-a47ff0439522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>date_key</th><th>full_date</th><th>date_name</th><th>date_name_us</th><th>date_name_eu</th><th>day_of_week</th><th>day_name_of_week</th><th>day_of_month</th><th>day_of_year</th><th>weekday_weekend</th><th>week_of_year</th><th>month_name</th><th>month_of_year</th><th>is_last_day_of_month</th><th>calendar_quarter</th><th>calendar_year</th><th>calendar_year_month</th><th>calendar_year_qtr</th><th>fiscal_month_of_year</th><th>fiscal_quarter</th><th>fiscal_year</th><th>fiscal_year_month</th><th>fiscal_year_qtr</th></tr></thead><tbody><tr><td>20000101</td><td>2000-01-01</td><td>2000/01/01</td><td>01/01/2000</td><td>01/01/2000</td><td>7</td><td>Saturday</td><td>1</td><td>1</td><td>Weekend</td><td>52</td><td>January</td><td>1</td><td>N</td><td>1</td><td>2000</td><td>2000-01</td><td>2000Q1</td><td>7</td><td>3</td><td>2000</td><td>2000-07</td><td>2000Q3</td></tr><tr><td>20000102</td><td>2000-01-02</td><td>2000/01/02</td><td>01/02/2000</td><td>02/01/2000</td><td>1</td><td>Sunday</td><td>2</td><td>2</td><td>Weekend</td><td>52</td><td>January</td><td>1</td><td>N</td><td>1</td><td>2000</td><td>2000-01</td><td>2000Q1</td><td>7</td><td>3</td><td>2000</td><td>2000-07</td><td>2000Q3</td></tr><tr><td>20000103</td><td>2000-01-03</td><td>2000/01/03</td><td>01/03/2000</td><td>03/01/2000</td><td>2</td><td>Monday</td><td>3</td><td>3</td><td>Weekday</td><td>1</td><td>January</td><td>1</td><td>N</td><td>1</td><td>2000</td><td>2000-01</td><td>2000Q1</td><td>7</td><td>3</td><td>2000</td><td>2000-07</td><td>2000Q3</td></tr><tr><td>20000104</td><td>2000-01-04</td><td>2000/01/04</td><td>01/04/2000</td><td>04/01/2000</td><td>3</td><td>Tuesday</td><td>4</td><td>4</td><td>Weekday</td><td>1</td><td>January</td><td>1</td><td>N</td><td>1</td><td>2000</td><td>2000-01</td><td>2000Q1</td><td>7</td><td>3</td><td>2000</td><td>2000-07</td><td>2000Q3</td></tr><tr><td>20000105</td><td>2000-01-05</td><td>2000/01/05</td><td>01/05/2000</td><td>05/01/2000</td><td>4</td><td>Wednesday</td><td>5</td><td>5</td><td>Weekday</td><td>1</td><td>January</td><td>1</td><td>N</td><td>1</td><td>2000</td><td>2000-01</td><td>2000Q1</td><td>7</td><td>3</td><td>2000</td><td>2000-07</td><td>2000Q3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         20000101,
         "2000-01-01",
         "2000/01/01",
         "01/01/2000",
         "01/01/2000",
         7,
         "Saturday",
         1,
         1,
         "Weekend",
         52,
         "January",
         1,
         "N",
         1,
         2000,
         "2000-01",
         "2000Q1",
         7,
         3,
         2000,
         "2000-07",
         "2000Q3"
        ],
        [
         20000102,
         "2000-01-02",
         "2000/01/02",
         "01/02/2000",
         "02/01/2000",
         1,
         "Sunday",
         2,
         2,
         "Weekend",
         52,
         "January",
         1,
         "N",
         1,
         2000,
         "2000-01",
         "2000Q1",
         7,
         3,
         2000,
         "2000-07",
         "2000Q3"
        ],
        [
         20000103,
         "2000-01-03",
         "2000/01/03",
         "01/03/2000",
         "03/01/2000",
         2,
         "Monday",
         3,
         3,
         "Weekday",
         1,
         "January",
         1,
         "N",
         1,
         2000,
         "2000-01",
         "2000Q1",
         7,
         3,
         2000,
         "2000-07",
         "2000Q3"
        ],
        [
         20000104,
         "2000-01-04",
         "2000/01/04",
         "01/04/2000",
         "04/01/2000",
         3,
         "Tuesday",
         4,
         4,
         "Weekday",
         1,
         "January",
         1,
         "N",
         1,
         2000,
         "2000-01",
         "2000Q1",
         7,
         3,
         2000,
         "2000-07",
         "2000Q3"
        ],
        [
         20000105,
         "2000-01-05",
         "2000/01/05",
         "01/05/2000",
         "05/01/2000",
         4,
         "Wednesday",
         5,
         5,
         "Weekday",
         1,
         "January",
         1,
         "N",
         1,
         2000,
         "2000-01",
         "2000Q1",
         7,
         3,
         2000,
         "2000-07",
         "2000Q3"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 49
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "date_key",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "full_date",
         "type": "\"date\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(11)\",\"signed\":true,\"scale\":0}",
         "name": "date_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(11)\",\"signed\":true,\"scale\":0}",
         "name": "date_name_us",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(11)\",\"signed\":true,\"scale\":0}",
         "name": "date_name_eu",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "day_of_week",
         "type": "\"byte\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(10)\",\"signed\":true,\"scale\":0}",
         "name": "day_name_of_week",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "day_of_month",
         "type": "\"byte\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "day_of_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(10)\",\"signed\":true,\"scale\":0}",
         "name": "weekday_weekend",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "week_of_year",
         "type": "\"byte\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(10)\",\"signed\":true,\"scale\":0}",
         "name": "month_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "month_of_year",
         "type": "\"byte\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(1)\",\"signed\":true,\"scale\":0}",
         "name": "is_last_day_of_month",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "calendar_quarter",
         "type": "\"byte\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "calendar_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(10)\",\"signed\":true,\"scale\":0}",
         "name": "calendar_year_month",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(10)\",\"signed\":true,\"scale\":0}",
         "name": "calendar_year_qtr",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "fiscal_month_of_year",
         "type": "\"byte\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "fiscal_quarter",
         "type": "\"byte\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "fiscal_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(10)\",\"signed\":true,\"scale\":0}",
         "name": "fiscal_year_month",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(10)\",\"signed\":true,\"scale\":0}",
         "name": "fiscal_year_qtr",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM northwind_dlh.dim_date LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c15f580a-989b-4a6a-bf5c-e50d3a8d3885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 1.3. Create a New Table that Sources Product Dimension Data from an Azure MySQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bb46df0-4476-4cda-86a1-f4cf614113fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Create a Temporary View named \"view_product\" that extracts data from your MySQL Northwind database.\n",
    "CREATE OR REPLACE TEMPORARY VIEW view_product\n",
    "USING org.apache.spark.sql.jdbc\n",
    "OPTIONS (\n",
    "  url \"jdbc:mysql://wna8fw-mysql.mysql.database.azure.com:3306/northwind_dw2\",\n",
    "  dbtable \"dim_products\", \n",
    "  user \"jtupitza\",    --Replace with your User Name\n",
    "  password \"Passw0rd123\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23d0585b-87f4-4f85-affa-3637c5e073f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 138
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE DATABASE northwind_dlh;\n",
    "\n",
    "-- Create a new table named \"northwind_dlh.dim_product\" using data from the view named \"view_product\"\n",
    "CREATE OR REPLACE TABLE northwind_dlh.dim_product \n",
    "COMMENT \"Product Dimension Table\"\n",
    "LOCATION \"dbfs:/FileStore/lab_data/northwind_dlh/dim_product\"\n",
    "AS SELECT * FROM view_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6acafaa-98f3-4b1d-9e14-3466c82c1e6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>product_key</td><td>bigint</td><td>null</td></tr><tr><td>product_id</td><td>bigint</td><td>null</td></tr><tr><td>product_code</td><td>varchar(65535)</td><td>null</td></tr><tr><td>product_name</td><td>varchar(65535)</td><td>null</td></tr><tr><td>standard_cost</td><td>double</td><td>null</td></tr><tr><td>list_price</td><td>double</td><td>null</td></tr><tr><td>reorder_level</td><td>bigint</td><td>null</td></tr><tr><td>target_level</td><td>bigint</td><td>null</td></tr><tr><td>quantity_per_unit</td><td>varchar(65535)</td><td>null</td></tr><tr><td>discontinued</td><td>bigint</td><td>null</td></tr><tr><td>minimum_reorder_quantity</td><td>double</td><td>null</td></tr><tr><td>category</td><td>varchar(65535)</td><td>null</td></tr><tr><td></td><td></td><td></td></tr><tr><td># Delta Statistics Columns</td><td></td><td></td></tr><tr><td>Column Names</td><td>list_price, standard_cost, target_level, discontinued, product_key, product_code, quantity_per_unit, product_name, reorder_level, minimum_reorder_quantity, product_id, category</td><td></td></tr><tr><td>Column Selection Method</td><td>first-32</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td># Detailed Table Information</td><td></td><td></td></tr><tr><td>Catalog</td><td>hive_metastore</td><td></td></tr><tr><td>Database</td><td>northwind_dlh</td><td></td></tr><tr><td>Table</td><td>dim_product</td><td></td></tr><tr><td>Created Time</td><td>Wed Nov 27 22:03:06 UTC 2024</td><td></td></tr><tr><td>Last Access</td><td>UNKNOWN</td><td></td></tr><tr><td>Created By</td><td>Spark 3.5.0</td><td></td></tr><tr><td>Statistics</td><td>13851 bytes</td><td></td></tr><tr><td>Type</td><td>EXTERNAL</td><td></td></tr><tr><td>Comment</td><td>Product Dimension Table</td><td></td></tr><tr><td>Location</td><td>dbfs:/FileStore/lab_data/northwind_dlh/dim_product</td><td></td></tr><tr><td>Provider</td><td>delta</td><td></td></tr><tr><td>Owner</td><td>root</td><td></td></tr><tr><td>Table Properties</td><td>[delta.enableDeletionVectors=true,delta.feature.deletionVectors=supported,delta.minReaderVersion=3,delta.minWriterVersion=7]</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "product_key",
         "bigint",
         null
        ],
        [
         "product_id",
         "bigint",
         null
        ],
        [
         "product_code",
         "varchar(65535)",
         null
        ],
        [
         "product_name",
         "varchar(65535)",
         null
        ],
        [
         "standard_cost",
         "double",
         null
        ],
        [
         "list_price",
         "double",
         null
        ],
        [
         "reorder_level",
         "bigint",
         null
        ],
        [
         "target_level",
         "bigint",
         null
        ],
        [
         "quantity_per_unit",
         "varchar(65535)",
         null
        ],
        [
         "discontinued",
         "bigint",
         null
        ],
        [
         "minimum_reorder_quantity",
         "double",
         null
        ],
        [
         "category",
         "varchar(65535)",
         null
        ],
        [
         "",
         "",
         ""
        ],
        [
         "# Delta Statistics Columns",
         "",
         ""
        ],
        [
         "Column Names",
         "list_price, standard_cost, target_level, discontinued, product_key, product_code, quantity_per_unit, product_name, reorder_level, minimum_reorder_quantity, product_id, category",
         ""
        ],
        [
         "Column Selection Method",
         "first-32",
         ""
        ],
        [
         "",
         "",
         ""
        ],
        [
         "# Detailed Table Information",
         "",
         ""
        ],
        [
         "Catalog",
         "hive_metastore",
         ""
        ],
        [
         "Database",
         "northwind_dlh",
         ""
        ],
        [
         "Table",
         "dim_product",
         ""
        ],
        [
         "Created Time",
         "Wed Nov 27 22:03:06 UTC 2024",
         ""
        ],
        [
         "Last Access",
         "UNKNOWN",
         ""
        ],
        [
         "Created By",
         "Spark 3.5.0",
         ""
        ],
        [
         "Statistics",
         "13851 bytes",
         ""
        ],
        [
         "Type",
         "EXTERNAL",
         ""
        ],
        [
         "Comment",
         "Product Dimension Table",
         ""
        ],
        [
         "Location",
         "dbfs:/FileStore/lab_data/northwind_dlh/dim_product",
         ""
        ],
        [
         "Provider",
         "delta",
         ""
        ],
        [
         "Owner",
         "root",
         ""
        ],
        [
         "Table Properties",
         "[delta.enableDeletionVectors=true,delta.feature.deletionVectors=supported,delta.minReaderVersion=3,delta.minWriterVersion=7]",
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 139
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\":\"name of the column\"}",
         "name": "col_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"data type of the column\"}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"comment of the column\"}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED northwind_dlh.dim_product;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc516608-0ca2-4c2f-b390-e56aca76b87d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_key</th><th>product_id</th><th>product_code</th><th>product_name</th><th>standard_cost</th><th>list_price</th><th>reorder_level</th><th>target_level</th><th>quantity_per_unit</th><th>discontinued</th><th>minimum_reorder_quantity</th><th>category</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>NWTB-1</td><td>Northwind Traders Chai</td><td>13.5</td><td>18.0</td><td>10</td><td>40</td><td>10 boxes x 20 bags</td><td>0</td><td>10.0</td><td>Beverages</td></tr><tr><td>2</td><td>3</td><td>NWTCO-3</td><td>Northwind Traders Syrup</td><td>7.5</td><td>10.0</td><td>25</td><td>100</td><td>12 - 550 ml bottles</td><td>0</td><td>25.0</td><td>Condiments</td></tr><tr><td>3</td><td>4</td><td>NWTCO-4</td><td>Northwind Traders Cajun Seasoning</td><td>16.5</td><td>22.0</td><td>10</td><td>40</td><td>48 - 6 oz jars</td><td>0</td><td>10.0</td><td>Condiments</td></tr><tr><td>4</td><td>5</td><td>NWTO-5</td><td>Northwind Traders Olive Oil</td><td>16.0125</td><td>21.35</td><td>10</td><td>40</td><td>36 boxes</td><td>0</td><td>10.0</td><td>Oil</td></tr><tr><td>5</td><td>6</td><td>NWTJP-6</td><td>Northwind Traders Boysenberry Spread</td><td>18.75</td><td>25.0</td><td>25</td><td>100</td><td>12 - 8 oz jars</td><td>0</td><td>25.0</td><td>Jams, Preserves</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         1,
         "NWTB-1",
         "Northwind Traders Chai",
         13.5,
         18.0,
         10,
         40,
         "10 boxes x 20 bags",
         0,
         10.0,
         "Beverages"
        ],
        [
         2,
         3,
         "NWTCO-3",
         "Northwind Traders Syrup",
         7.5,
         10.0,
         25,
         100,
         "12 - 550 ml bottles",
         0,
         25.0,
         "Condiments"
        ],
        [
         3,
         4,
         "NWTCO-4",
         "Northwind Traders Cajun Seasoning",
         16.5,
         22.0,
         10,
         40,
         "48 - 6 oz jars",
         0,
         10.0,
         "Condiments"
        ],
        [
         4,
         5,
         "NWTO-5",
         "Northwind Traders Olive Oil",
         16.0125,
         21.35,
         10,
         40,
         "36 boxes",
         0,
         10.0,
         "Oil"
        ],
        [
         5,
         6,
         "NWTJP-6",
         "Northwind Traders Boysenberry Spread",
         18.75,
         25.0,
         25,
         100,
         "12 - 8 oz jars",
         0,
         25.0,
         "Jams, Preserves"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 140
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "product_key",
         "type": "\"long\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "product_id",
         "type": "\"long\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(65535)\",\"signed\":true,\"scale\":0}",
         "name": "product_code",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(65535)\",\"signed\":true,\"scale\":0}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":31}",
         "name": "standard_cost",
         "type": "\"double\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":31}",
         "name": "list_price",
         "type": "\"double\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "reorder_level",
         "type": "\"long\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "target_level",
         "type": "\"long\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(65535)\",\"signed\":true,\"scale\":0}",
         "name": "quantity_per_unit",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "discontinued",
         "type": "\"long\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":31}",
         "name": "minimum_reorder_quantity",
         "type": "\"double\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(65535)\",\"signed\":true,\"scale\":0}",
         "name": "category",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM northwind_dlh.dim_product LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5763da8c-b2df-4cfa-83e6-0e05503ddce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.0. Fetch Reference Data from a MongoDB Atlas Database\n",
    "##### 2.1. View the Data Files on the Databricks File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd0ce73-5fc9-48b1-9ee8-0ffaf402407f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/lab_data/retail/batch/Northwind_DimCustomers-1.json</td><td>Northwind_DimCustomers-1.json</td><td>10476</td><td>1732738833000</td></tr><tr><td>dbfs:/FileStore/lab_data/retail/batch/Northwind_DimCustomers-2.json</td><td>Northwind_DimCustomers-2.json</td><td>10476</td><td>1732738909000</td></tr><tr><td>dbfs:/FileStore/lab_data/retail/batch/Northwind_DimCustomers.json</td><td>Northwind_DimCustomers.json</td><td>10476</td><td>1732729125000</td></tr><tr><td>dbfs:/FileStore/lab_data/retail/batch/Northwind_DimEmployees-1.csv</td><td>Northwind_DimEmployees-1.csv</td><td>2164</td><td>1732738833000</td></tr><tr><td>dbfs:/FileStore/lab_data/retail/batch/Northwind_DimEmployees-2.csv</td><td>Northwind_DimEmployees-2.csv</td><td>2164</td><td>1732738909000</td></tr><tr><td>dbfs:/FileStore/lab_data/retail/batch/Northwind_DimEmployees.csv</td><td>Northwind_DimEmployees.csv</td><td>2164</td><td>1732729125000</td></tr><tr><td>dbfs:/FileStore/lab_data/retail/batch/Northwind_DimInvoices-1.json</td><td>Northwind_DimInvoices-1.json</td><td>6263</td><td>1732738833000</td></tr><tr><td>dbfs:/FileStore/lab_data/retail/batch/Northwind_DimInvoices-2.json</td><td>Northwind_DimInvoices-2.json</td><td>6263</td><td>1732738909000</td></tr><tr><td>dbfs:/FileStore/lab_data/retail/batch/Northwind_DimInvoices.json</td><td>Northwind_DimInvoices.json</td><td>6263</td><td>1732729125000</td></tr><tr><td>dbfs:/FileStore/lab_data/retail/batch/Northwind_DimShippers-1.csv</td><td>Northwind_DimShippers-1.csv</td><td>262</td><td>1732738833000</td></tr><tr><td>dbfs:/FileStore/lab_data/retail/batch/Northwind_DimShippers-2.csv</td><td>Northwind_DimShippers-2.csv</td><td>262</td><td>1732738909000</td></tr><tr><td>dbfs:/FileStore/lab_data/retail/batch/Northwind_DimShippers.csv</td><td>Northwind_DimShippers.csv</td><td>262</td><td>1732729125000</td></tr><tr><td>dbfs:/FileStore/lab_data/retail/batch/Northwind_DimSuppliers-1.json</td><td>Northwind_DimSuppliers-1.json</td><td>1480</td><td>1732738833000</td></tr><tr><td>dbfs:/FileStore/lab_data/retail/batch/Northwind_DimSuppliers-2.json</td><td>Northwind_DimSuppliers-2.json</td><td>1480</td><td>1732738909000</td></tr><tr><td>dbfs:/FileStore/lab_data/retail/batch/Northwind_DimSuppliers.json</td><td>Northwind_DimSuppliers.json</td><td>1480</td><td>1732729125000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/lab_data/retail/batch/Northwind_DimCustomers-1.json",
         "Northwind_DimCustomers-1.json",
         10476,
         1732738833000
        ],
        [
         "dbfs:/FileStore/lab_data/retail/batch/Northwind_DimCustomers-2.json",
         "Northwind_DimCustomers-2.json",
         10476,
         1732738909000
        ],
        [
         "dbfs:/FileStore/lab_data/retail/batch/Northwind_DimCustomers.json",
         "Northwind_DimCustomers.json",
         10476,
         1732729125000
        ],
        [
         "dbfs:/FileStore/lab_data/retail/batch/Northwind_DimEmployees-1.csv",
         "Northwind_DimEmployees-1.csv",
         2164,
         1732738833000
        ],
        [
         "dbfs:/FileStore/lab_data/retail/batch/Northwind_DimEmployees-2.csv",
         "Northwind_DimEmployees-2.csv",
         2164,
         1732738909000
        ],
        [
         "dbfs:/FileStore/lab_data/retail/batch/Northwind_DimEmployees.csv",
         "Northwind_DimEmployees.csv",
         2164,
         1732729125000
        ],
        [
         "dbfs:/FileStore/lab_data/retail/batch/Northwind_DimInvoices-1.json",
         "Northwind_DimInvoices-1.json",
         6263,
         1732738833000
        ],
        [
         "dbfs:/FileStore/lab_data/retail/batch/Northwind_DimInvoices-2.json",
         "Northwind_DimInvoices-2.json",
         6263,
         1732738909000
        ],
        [
         "dbfs:/FileStore/lab_data/retail/batch/Northwind_DimInvoices.json",
         "Northwind_DimInvoices.json",
         6263,
         1732729125000
        ],
        [
         "dbfs:/FileStore/lab_data/retail/batch/Northwind_DimShippers-1.csv",
         "Northwind_DimShippers-1.csv",
         262,
         1732738833000
        ],
        [
         "dbfs:/FileStore/lab_data/retail/batch/Northwind_DimShippers-2.csv",
         "Northwind_DimShippers-2.csv",
         262,
         1732738909000
        ],
        [
         "dbfs:/FileStore/lab_data/retail/batch/Northwind_DimShippers.csv",
         "Northwind_DimShippers.csv",
         262,
         1732729125000
        ],
        [
         "dbfs:/FileStore/lab_data/retail/batch/Northwind_DimSuppliers-1.json",
         "Northwind_DimSuppliers-1.json",
         1480,
         1732738833000
        ],
        [
         "dbfs:/FileStore/lab_data/retail/batch/Northwind_DimSuppliers-2.json",
         "Northwind_DimSuppliers-2.json",
         1480,
         1732738909000
        ],
        [
         "dbfs:/FileStore/lab_data/retail/batch/Northwind_DimSuppliers.json",
         "Northwind_DimSuppliers.json",
         1480,
         1732729125000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(batch_dir))  # '/dbfs/FileStore/lab_data/retail/batch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e943953-1c04-43a3-b117-01980bfefbf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 2.2. Create a New MongoDB Database, and Load JSON Data Into a New MongoDB Collection\n",
    "**NOTE:** The following cell **can** be run more than once because the **set_mongo_collection()** function **is** idempotent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cab75353-e39e-4fde-b1fb-068a7c914cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "uri = \"mongodb+srv://jsq4xr:5DltCXGlClkT1XtZ@cluster0.laoqh.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12764988-6562-4070-af68-697ca403bfe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "InsertManyResult([ObjectId('674798b10240b2ac269af7dc'), ObjectId('674798b10240b2ac269af7dd'), ObjectId('674798b10240b2ac269af7de'), ObjectId('674798b10240b2ac269af7df'), ObjectId('674798b10240b2ac269af7e0'), ObjectId('674798b10240b2ac269af7e1'), ObjectId('674798b10240b2ac269af7e2'), ObjectId('674798b10240b2ac269af7e3'), ObjectId('674798b10240b2ac269af7e4'), ObjectId('674798b10240b2ac269af7e5'), ObjectId('674798b10240b2ac269af7e6'), ObjectId('674798b10240b2ac269af7e7'), ObjectId('674798b10240b2ac269af7e8'), ObjectId('674798b10240b2ac269af7e9'), ObjectId('674798b10240b2ac269af7ea'), ObjectId('674798b10240b2ac269af7eb'), ObjectId('674798b10240b2ac269af7ec'), ObjectId('674798b10240b2ac269af7ed'), ObjectId('674798b10240b2ac269af7ee'), ObjectId('674798b10240b2ac269af7ef'), ObjectId('674798b10240b2ac269af7f0'), ObjectId('674798b10240b2ac269af7f1'), ObjectId('674798b10240b2ac269af7f2'), ObjectId('674798b10240b2ac269af7f3'), ObjectId('674798b10240b2ac269af7f4'), ObjectId('674798b10240b2ac269af7f5'), ObjectId('674798b10240b2ac269af7f6'), ObjectId('674798b10240b2ac269af7f7'), ObjectId('674798b10240b2ac269af7f8'), ObjectId('674798b10240b2ac269af7f9'), ObjectId('674798b10240b2ac269af7fa'), ObjectId('674798b10240b2ac269af7fb'), ObjectId('674798b10240b2ac269af7fc'), ObjectId('674798b10240b2ac269af7fd'), ObjectId('674798b10240b2ac269af7fe')], acknowledged=True)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dir = '/dbfs/FileStore/lab_data/retail/batch'\n",
    "json_files = {\"customers\" : 'Northwind_DimCustomers.json'\n",
    "              , \"suppliers\" : 'Northwind_DimSuppliers.json'\n",
    "              , \"invoices\" : 'Northwind_DimInvoices.json'}\n",
    "\n",
    "set_mongo_collection(atlas_user_name, atlas_password, \"Cluster0\", atlas_database_name, source_dir, json_files) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "675d2e83-b51d-48ef-937d-f8beacb86fb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 2.3.1. Fetch Customer Dimension Data from the New MongoDB Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1908f9a0-b4a7-4127-a0a3-3d89ee35a500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.0\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0513c095-c4ba-4820-beec-5780f730a57c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n",
       "\u001B[0;32m<command-924093559460666>\u001B[0m in \u001B[0;36m<cell line: 14>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     13\u001B[0m \u001B[0;31m# Read data from MongoDB (replace 'your_database.your_collection' with actual values)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 14\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"mongo\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"uri\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmongo_uri\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     16\u001B[0m \u001B[0;31m# Show the data to confirm the connection is working\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n",
       "\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n",
       "\u001B[1;32m    182\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    183\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 184\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    185\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    186\u001B[0m     def json(\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n",
       "\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m: Missing database name. Set via the 'spark.mongodb.input.uri' or 'spark.mongodb.input.database' property"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-924093559460666>\u001B[0m in \u001B[0;36m<cell line: 14>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;31m# Read data from MongoDB (replace 'your_database.your_collection' with actual values)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"mongo\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"uri\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmongo_uri\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;31m# Show the data to confirm the connection is working\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    182\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    183\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 184\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    185\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    186\u001B[0m     def json(\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: Missing database name. Set via the 'spark.mongodb.input.uri' or 'spark.mongodb.input.database' property",
       "errorSummary": "<span class='ansi-red-fg'>IllegalArgumentException</span>: Missing database name. Set via the 'spark.mongodb.input.uri' or 'spark.mongodb.input.database' property",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# MongoDB URI and other configurations\n",
    "mongo_uri = \"mongodb+srv://jsq4xr:5DltCXGlClkT1XtZ@cluster0.laoqh.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n",
    "# Initialize Spark session with MongoDB configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoDBSparkConnectorExample\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", mongo_uri) \\\n",
    "    .config(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from MongoDB (replace 'your_database.your_collection' with actual values)\n",
    "df = spark.read.format(\"mongo\").option(\"uri\", mongo_uri).load()\n",
    "\n",
    "# Show the data to confirm the connection is working\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4881e786-3560-425b-8a1a-8601e8de2bf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<div class=\"ansiout\">command-997251033567225:1: error: object mongodb is not a member of package com\nimport com.mongodb.spark.sql._\n           ^\n</div>",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "import com.mongodb.spark.sql._\n",
    "\n",
    "val userName = \"jsq4xr\"\n",
    "val pwd = \"5DltCXGlClkT1XtZ\"\n",
    "val clusterName = \"Cluster0\"\n",
    "val atlas_uri = s\"mongodb+srv://$userName:$pwd@$clusterName.mongodb.net/?retryWrites=true&w=majority\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5029015-be65-49ed-b2e7-eb60d22013ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:91)\n",
       "\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:398)\n",
       "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:394)\n",
       "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:350)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:350)\n",
       "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:236)\n",
       "\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-997251033567226:5)\n",
       "\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-997251033567226:55)\n",
       "\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-997251033567226:57)\n",
       "\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read$$iw$$iw$$iw.&lt;init&gt;(command-997251033567226:59)\n",
       "\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read$$iw$$iw.&lt;init&gt;(command-997251033567226:61)\n",
       "\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read$$iw.&lt;init&gt;(command-997251033567226:63)\n",
       "\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read.&lt;init&gt;(command-997251033567226:65)\n",
       "\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read$.&lt;init&gt;(command-997251033567226:69)\n",
       "\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read$.&lt;clinit&gt;(command-997251033567226)\n",
       "\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n",
       "\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$eval$.$print(&lt;notebook&gt;:6)\n",
       "\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$eval.$print(&lt;notebook&gt;)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n",
       "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n",
       "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n",
       "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n",
       "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n",
       "\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:201)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$3(ScalaDriverLocal.scala:295)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.threadSafeTrapExit(DriverLocal.scala:1609)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1567)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1491)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.executeCommand$1(ScalaDriverLocal.scala:295)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$2(ScalaDriverLocal.scala:265)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat scala.Console$.withErr(Console.scala:196)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:262)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat scala.Console$.withOut(Console.scala:167)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:262)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1172)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$28(DriverLocal.scala:1163)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:96)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1099)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1519)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:776)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:932)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:921)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:967)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:73)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:73)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:73)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:967)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:717)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:785)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:590)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:73)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:590)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:512)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:306)\n",
       "\tat java.lang.Thread.run(Thread.java:750)</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\">\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:91)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:398)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:394)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:350)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:236)\n\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-997251033567226:5)\n\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-997251033567226:55)\n\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-997251033567226:57)\n\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read$$iw$$iw$$iw.&lt;init&gt;(command-997251033567226:59)\n\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read$$iw$$iw.&lt;init&gt;(command-997251033567226:61)\n\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read$$iw.&lt;init&gt;(command-997251033567226:63)\n\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read.&lt;init&gt;(command-997251033567226:65)\n\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read$.&lt;init&gt;(command-997251033567226:69)\n\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$read$.&lt;clinit&gt;(command-997251033567226)\n\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$eval$.$print(&lt;notebook&gt;:6)\n\tat $linec0cb592988fa4e088d3c11d53ffd543c72.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:201)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$3(ScalaDriverLocal.scala:295)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.threadSafeTrapExit(DriverLocal.scala:1609)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1567)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1491)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.executeCommand$1(ScalaDriverLocal.scala:295)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$2(ScalaDriverLocal.scala:265)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat scala.Console$.withErr(Console.scala:196)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:262)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat scala.Console$.withOut(Console.scala:167)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:262)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1172)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$28(DriverLocal.scala:1163)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:96)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1099)\n\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1519)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:776)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:932)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:921)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:967)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:73)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:73)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:73)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:967)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:717)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:785)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:590)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:73)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:590)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:512)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:306)\n\tat java.lang.Thread.run(Thread.java:750)</div>",
       "errorSummary": "NoClassDefFoundError: Could not initialize class com.mongodb.spark.config.ReadConfig$",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "\n",
    "val df_customer = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    ".option(\"spark.mongodb.input.uri\", atlas_uri)\n",
    ".option(\"database\", \"northwind_dw2\")\n",
    ".option(\"collection\", \"customers\").load()\n",
    ".select(\"customer_key\",\"company\",\"last_name\",\"first_name\",\"job_title\",\"business_phone\",\"fax_number\",\"address\",\"city\",\"state_province\",\"zip_postal_code\",\"country_region\")\n",
    "\n",
    "display(df_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d62a4b0-3071-400f-8bbb-5a89f3c91d5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<div class=\"ansiout\">command-997251033567227:1: error: not found: value df_customer\ndf_customer.printSchema()\n^\n</div>",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "df_customer.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "179f0875-98bd-4988-bfa5-af82d0181eb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 2.3.2. Use the Spark DataFrame to Create a New Customer Dimension Table in the Databricks Metadata Database (northwind_dlh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de15218-dfa0-4ec1-ab3f-243d650b86b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<div class=\"ansiout\">command-997251033567229:1: error: not found: value df_customer\ndf_customer.write.format(&quot;delta&quot;).mode(&quot;overwrite&quot;).saveAsTable(&quot;northwind_dlh.dim_customer&quot;)\n^\n</div>",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "df_customer.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"northwind_dlh.dim_customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62999814-4e3e-4bb1-aac1-7ef79266c22d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/FileStore/lab_data/northwind_dlh/dim_customer doesn't exist, or is not a Delta table.;\n",
       "DescribeRelation true, [col_name#8339, data_type#8340, comment#8341]\n",
       "+- ResolvedTable com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@63652ae6, northwind_dlh.dim_customer, DeltaTableV2(org.apache.spark.sql.SparkSession@7298c35,dbfs:/FileStore/lab_data/northwind_dlh/dim_customer,Some(CatalogTable(\n",
       "Catalog: hive_metastore\n",
       "Database: northwind_dlh\n",
       "Table: dim_customer\n",
       "Owner: root\n",
       "Created Time: Wed Nov 27 22:03:38 UTC 2024\n",
       "Last Access: UNKNOWN\n",
       "Created By: Spark 3.5.0\n",
       "Type: MANAGED\n",
       "Provider: delta\n",
       "Table Properties: [delta.enableDeletionVectors=true, delta.feature.deletionVectors=supported, delta.lastCommitTimestamp=1732745017000, delta.lastUpdateVersion=0, delta.minReaderVersion=3, delta.minWriterVersion=7]\n",
       "Location: dbfs:/FileStore/lab_data/northwind_dlh/dim_customer\n",
       "Serde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n",
       "InputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\n",
       "OutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n",
       "Partition Provider: Catalog\n",
       "Schema: root\n",
       " |-- customer_key: integer (nullable = true)\n",
       " |-- company: string (nullable = true)\n",
       " |-- last_name: string (nullable = true)\n",
       " |-- first_name: string (nullable = true)\n",
       " |-- job_title: string (nullable = true)\n",
       " |-- business_phone: string (nullable = true)\n",
       " |-- fax_number: string (nullable = true)\n",
       " |-- address: string (nullable = true)\n",
       " |-- city: string (nullable = true)\n",
       " |-- state_province: string (nullable = true)\n",
       " |-- zip_postal_code: string (nullable = true)\n",
       " |-- country_region: string (nullable = true)\n",
       ")),Some(hive_metastore.northwind_dlh.dim_customer),None,Map())\n",
       "\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.pathNotExistsException(DeltaErrors.scala:857)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.pathNotExistsException$(DeltaErrors.scala:856)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.pathNotExistsException(DeltaErrors.scala:3573)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaUnsupportedOperationsCheck.$anonfun$apply$1(DeltaUnsupportedOperationsCheck.scala:133)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaUnsupportedOperationsCheck.$anonfun$apply$1$adapted(DeltaUnsupportedOperationsCheck.scala:72)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:277)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreach$1(TreeNode.scala:278)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreach$1$adapted(TreeNode.scala:278)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
       "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
       "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:278)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaUnsupportedOperationsCheck.apply(DeltaUnsupportedOperationsCheck.scala:72)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaUnsupportedOperationsCheck.apply(DeltaUnsupportedOperationsCheck.scala:48)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$56(CheckAnalysis.scala:820)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$56$adapted(CheckAnalysis.scala:820)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:820)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:179)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:369)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:167)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:155)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:155)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:369)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:424)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:178)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:424)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:421)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:262)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:454)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:613)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:144)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:613)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1177)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:612)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:608)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:608)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:255)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:237)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:130)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1187)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1187)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:122)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:959)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:947)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:982)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:301)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:401)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:423)\n",
       "\tat scala.collection.immutable.List.map(List.scala:293)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:1053)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1172)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$28(DriverLocal.scala:1163)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:96)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1099)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1519)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:776)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:932)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:921)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:967)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:73)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:73)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:73)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:967)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:717)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:785)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:590)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:73)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:590)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:512)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:306)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "[DELTA_PATH_DOES_NOT_EXIST] dbfs:/FileStore/lab_data/northwind_dlh/dim_customer doesn't exist, or is not a Delta table. SQLSTATE: 42K03"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "DELTA_PATH_DOES_NOT_EXIST",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42K03",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/FileStore/lab_data/northwind_dlh/dim_customer doesn't exist, or is not a Delta table.;\nDescribeRelation true, [col_name#8339, data_type#8340, comment#8341]\n+- ResolvedTable com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@63652ae6, northwind_dlh.dim_customer, DeltaTableV2(org.apache.spark.sql.SparkSession@7298c35,dbfs:/FileStore/lab_data/northwind_dlh/dim_customer,Some(CatalogTable(\nCatalog: hive_metastore\nDatabase: northwind_dlh\nTable: dim_customer\nOwner: root\nCreated Time: Wed Nov 27 22:03:38 UTC 2024\nLast Access: UNKNOWN\nCreated By: Spark 3.5.0\nType: MANAGED\nProvider: delta\nTable Properties: [delta.enableDeletionVectors=true, delta.feature.deletionVectors=supported, delta.lastCommitTimestamp=1732745017000, delta.lastUpdateVersion=0, delta.minReaderVersion=3, delta.minWriterVersion=7]\nLocation: dbfs:/FileStore/lab_data/northwind_dlh/dim_customer\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog\nSchema: root\n |-- customer_key: integer (nullable = true)\n |-- company: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- job_title: string (nullable = true)\n |-- business_phone: string (nullable = true)\n |-- fax_number: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state_province: string (nullable = true)\n |-- zip_postal_code: string (nullable = true)\n |-- country_region: string (nullable = true)\n)),Some(hive_metastore.northwind_dlh.dim_customer),None,Map())\n\n\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.pathNotExistsException(DeltaErrors.scala:857)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.pathNotExistsException$(DeltaErrors.scala:856)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.pathNotExistsException(DeltaErrors.scala:3573)\n\tat com.databricks.sql.transaction.tahoe.DeltaUnsupportedOperationsCheck.$anonfun$apply$1(DeltaUnsupportedOperationsCheck.scala:133)\n\tat com.databricks.sql.transaction.tahoe.DeltaUnsupportedOperationsCheck.$anonfun$apply$1$adapted(DeltaUnsupportedOperationsCheck.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreach$1(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreach$1$adapted(TreeNode.scala:278)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:278)\n\tat com.databricks.sql.transaction.tahoe.DeltaUnsupportedOperationsCheck.apply(DeltaUnsupportedOperationsCheck.scala:72)\n\tat com.databricks.sql.transaction.tahoe.DeltaUnsupportedOperationsCheck.apply(DeltaUnsupportedOperationsCheck.scala:48)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$56(CheckAnalysis.scala:820)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$56$adapted(CheckAnalysis.scala:820)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:820)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:179)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:369)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:155)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:155)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:369)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:424)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:178)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:424)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:421)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:262)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:454)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:613)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:613)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1177)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:612)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:608)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:608)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:256)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:255)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:237)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:130)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1187)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1187)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:122)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:959)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:947)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:982)\n\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:301)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:401)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:423)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:418)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:1053)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1172)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$28(DriverLocal.scala:1163)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:96)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1099)\n\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1519)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:776)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:932)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:921)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:967)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:73)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:73)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:73)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:967)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:717)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:785)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:590)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:73)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:590)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:512)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:306)\n\tat java.lang.Thread.run(Thread.java:750)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED northwind_dlh.dim_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4924a30-ad66-424c-b511-19cbfaefe93e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM northwind_dlh.dim_customer LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b52ecae-78cd-4dce-a974-f052dbe9b9bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 2.4.1 Fetch Supplier Dimension Data from the New MongoDB Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73350185-ce36-49ed-86e0-041f2d12d289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val df_supplier = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    ".option(\"spark.mongodb.input.uri\", atlas_uri)\n",
    ".option(\"database\", \"northwind_dw2\")\n",
    ".option(\"collection\", \"suppliers\").load()\n",
    ".select(\"supplier_key\",\"company\",\"contact_name\",\"contact_title\",\"address\",\"city\",\"state_province\",\"zip_postal_code\",\"country_region\")\n",
    "\n",
    "df_supplier.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"northwind_dlh.dim_supplier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a37d21b2-511b-4553-837a-b957094d9db8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "df_supplier.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca79d2c6-6a92-460d-b163-cdbe81c555a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 2.4.2. Use the Spark DataFrame to Create a New Suppliers Dimension Table in the Databricks Metadata Database (northwind_dlh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0766bc0c-2e6c-4d3e-8879-b9b4fae5f3d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "shipper_csv = f\"{batch_dir}/Northwind_DimShippers.csv\"\n",
    "\n",
    "df_shipper = spark.read.format('csv').options(header='true', inferSchema='true').load(shipper_csv)\n",
    "df_shipper.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"northwind_dlh.dim_shipper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfbe73eb-d32b-4685-aa6a-ecce737d8b63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED northwind_dlh.dim_supplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c3d611c-95e5-406d-b8da-dd17e797a018",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM northwind_dlh.dim_supplier LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dca49d6-6644-4968-b821-492ad3bd3003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 2.5.1 Fetch Invoice Dimension Data from teh New MongoDB Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9caebb6-5c7f-4cc7-998b-0044bdcfa8b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val df_invoice = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    ".option(\"spark.mongodb.input.uri\", atlas_uri)\n",
    ".option(\"database\", \"northwind_dw2\")\n",
    ".option(\"collection\", \"invoices\").load()\n",
    ".select(\"invoice_key\",\n",
    "        \"invoice_date_key\",\n",
    "        \"invoice_number\", \n",
    "        \"due_date_key\",\n",
    "        \"payment_date_key\",\n",
    "        \"order_key\",\n",
    "        \"customer_key\",\n",
    "        \"shipper_key\",\n",
    "        \"product_key\",\n",
    "        \"shipping_fee\",\n",
    "        \"taxes\",\n",
    "        \"payment_type\",\n",
    "        \"paid_date_key\",\n",
    "        \"notes\",\n",
    "        \"tax_rate\",\n",
    "        \"freight\",\n",
    "        \"order_date_key\")\n",
    "\n",
    "display(df_invoice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb0cce12-7995-4ad6-9e3f-0148260c3244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "df_invoice.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "728760ad-f138-4e2f-bba1-80b79eccb257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 2.5.2. Use the Spark DataFrame to Create a New Invoices Dimension Table in the Databricks Metadata Database (northwind_dlh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "736a6a8d-19b8-43a0-ab42-1e5a49bfea8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val df_invoice = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    ".option(\"spark.mongodb.input.uri\", atlas_uri)\n",
    ".option(\"database\", \"northwind_dw2\")\n",
    ".option(\"collection\", \"invoices\").load()\n",
    ".select(\"invoice_key\",\n",
    "        \"invoice_date_key\",\n",
    "        \"invoice_number\", \n",
    "        \"due_date_key\",\n",
    "        \"payment_date_key\",\n",
    "        \"order_key\",\n",
    "        \"customer_key\",\n",
    "        \"shipper_key\",\n",
    "        \"product_key\",\n",
    "        \"shipping_fee\",\n",
    "        \"taxes\",\n",
    "        \"payment_type\",\n",
    "        \"paid_date_key\",\n",
    "        \"notes\",\n",
    "        \"tax_rate\",\n",
    "        \"freight\",\n",
    "        \"order_date_key\")\n",
    "\n",
    "df_invoice.write.format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .saveAsTable(\"northwind_dlh.dim_invoice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a260fd6-e930-4e66-872d-6dda13c9283e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED northwind_dlh.dim_invoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030bba66-4265-493e-93c9-b6449d29ee16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM northwind_dlh.dim_invoice LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9936b98e-53e2-4f75-9db5-9bbefad735fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3.0. Fetch Data from a File System\n",
    "##### 3.1. Use PySpark to Read From a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b47c7ad8-a57b-46ca-a734-9bbf271cf0bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_csv = f\"{batch_dir}/Northwind_DimEmployees.csv\"\n",
    "\n",
    "df_employee = spark.read.format('csv').options(header='true', inferSchema='true').load(employee_csv)\n",
    "display(df_employee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12852bc5-3ebd-4ef8-a96b-8b25516ccbea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employee.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "199f4806-101a-41bf-ac54-b8ec2b8c4935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employee.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"northwind_dlh.dim_employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb7e6ae7-5c04-4c46-a49f-bbdaa975ccc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED northwind_dlh.dim_employee;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0e93baa-868a-4ea0-b168-df44c2527f17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM northwind_dlh.dim_employee LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f16bed2-735a-4fb4-a8e3-14952976a618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3.2 Use PySpark to Read Shipper Dimension Data from CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75129b67-0db6-48cb-8e3b-0323cc4a429a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shipper_csv = f\"{batch_dir}/Northwind_DimShippers.csv\"\n",
    "\n",
    "df_shipper = spark.read.format('csv') \\\n",
    "    .options(header='true', inferSchema='true') \\\n",
    "    .load(shipper_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2a25d2a-481e-4022-acbb-425460b5b8cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_shipper.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36999c8d-4ce0-47f8-b6bc-d40817d135dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_shipper.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"northwind_dlh.dim_shipper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3f857ae-fedf-4984-ae51-a5006fe774d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED northwind_dlh.dim_shipper;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da35ad65-400d-45f1-928f-dfeccfccf359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM northwind_dlh.dim_shipper LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10b2930e-732d-409a-9853-28738802fac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Verify Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da830d5-ecdd-4b6f-bc43-7c7559e9d34e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE northwind_dlh;\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cd1e0df-ee40-4631-95f2-fecd438276d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Section III: Integrate Reference Data with Real-Time Data\n",
    "#### 6.0. Use AutoLoader to Process Streaming (Hot Path) Orders Fact Data \n",
    "##### 6.1. Bronze Table: Process 'Raw' JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34000bc1-7f1b-4d21-bfd2-10e2bf9ee979",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream\n",
    " .format(\"cloudFiles\")\n",
    " .option(\"cloudFiles.format\", \"json\")\n",
    " #.option(\"cloudFiles.schemaHints\", \"fact_order_key BIGINT\")\n",
    " #.option(\"cloudFiles.schemaHints\", \"order_key BIGINT\")\n",
    " #.option(\"cloudFiles.schemaHints\", \"employee_key BIGINT\")\n",
    " #.option(\"cloudFiles.schemaHints\", \"customer_key BIGINT\") \n",
    " #.option(\"cloudFiles.schemaHints\", \"product_key BIGINT\")\n",
    " #.option(\"cloudFiles.schemaHints\", \"shipper_key DECIMAL\")\n",
    " #.option(\"cloudFiles.schemaHints\", \"order_date_key DECIMAL\")\n",
    " #.option(\"cloudFiles.schemaHints\", \"paid_date_key DECIMAL\")\n",
    " #.option(\"cloudFiles.schemaHints\", \"shipped_date_key DECIMAL\") \n",
    " #.option(\"cloudFiles.schemaHints\", \"quantity DECIMAL\")\n",
    " #.option(\"cloudFiles.schemaHints\", \"unit_price DECIMAL\")\n",
    " #.option(\"cloudFiles.schemaHints\", \"discount DECIMAL\")\n",
    " #.option(\"cloudFiles.schemaHints\", \"shipping_fee DECIMAL\")\n",
    " #.option(\"cloudFiles.schemaHints\", \"taxes DECIMAL\")\n",
    " #.option(\"cloudFiles.schemaHints\", \"tax_rate DECIMAL\")\n",
    " #.option(\"cloudFiles.schemaHints\", \"payment_type STRING\")\n",
    " #.option(\"cloudFiles.schemaHints\", \"order_status STRING\")\n",
    " #.option(\"cloudFiles.schemaHints\", \"order_details_status STRING\")\n",
    " .option(\"cloudFiles.schemaLocation\", orders_output_bronze)\n",
    " .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    " .option(\"multiLine\", \"true\")\n",
    " .load(orders_stream_dir)\n",
    " .createOrReplaceTempView(\"orders_raw_tempview\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acf1fa4c-6860-4a4b-9046-bd79a738891b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "/* Add Metadata for Traceability */\n",
    "CREATE OR REPLACE TEMPORARY VIEW orders_bronze_tempview AS (\n",
    "  SELECT *, current_timestamp() receipt_time, input_file_name() source_file\n",
    "  FROM orders_raw_tempview\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ecede6-693c-4949-8aba-b2344680af03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM orders_bronze_tempview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e89a9e0c-69da-452a-92d5-724fb81529ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.table(\"orders_bronze_tempview\")\n",
    "      .writeStream\n",
    "      .format(\"delta\")\n",
    "      .option(\"checkpointLocation\", f\"{orders_output_bronze}/_checkpoint\")\n",
    "      .outputMode(\"append\")\n",
    "      .table(\"fact_orders_bronze\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "030a8ad8-1937-4708-91e2-2975eb731862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 6.2. Silver Table: Include Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4998a27c-4486-4d7c-bb79-f29d7cdeae01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream\n",
    "  .table(\"fact_orders_bronze\")\n",
    "  .createOrReplaceTempView(\"orders_silver_tempview\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b160aaaf-2edc-46eb-8514-765d55695ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM orders_silver_tempview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e95e9e-62c2-4c85-a66b-7d6e145a4193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED orders_silver_tempview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710c49bc-c34a-4db5-9cea-ab801ffe9688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW fact_orders_silver_tempview AS (\n",
    "  SELECT o.fact_order_key,\n",
    "      o.order_key,\n",
    "      o.employee_key,\n",
    "      e.last_name AS employee_last_name,\n",
    "      e.first_name AS employee_first_name,\n",
    "      e.job_title AS employee_job_title,\n",
    "      e.company AS employee_company,\n",
    "      o.customer_key,\n",
    "      c.last_name AS customer_last_name,\n",
    "      c.first_name AS customer_first_name,\n",
    "      o.product_key,\n",
    "      p.product_code,\n",
    "      p.product_name,\n",
    "      p.standard_cost AS product_standard_cost,\n",
    "      p.list_price AS product_list_price,\n",
    "      p.category AS product_category,\n",
    "      o.shipper_key,\n",
    "      s.company AS shipper_company,\n",
    "      s.state_province AS shipper_state_province,\n",
    "      s.country_region AS shipper_country_region,\n",
    "      o.order_date_key,\n",
    "      od.day_name_of_week AS order_day_name_of_week,\n",
    "      od.day_of_month AS order_day_of_month,\n",
    "      od.weekday_weekend AS order_weekday_weekend,\n",
    "      od.month_name AS order_month_name,\n",
    "      od.calendar_quarter AS order_quarter,\n",
    "      od.calendar_year AS order_year,\n",
    "      o.paid_date_key,\n",
    "      pd.day_name_of_week AS paid_day_name_of_week,\n",
    "      pd.day_of_month AS paid_day_of_month,\n",
    "      pd.weekday_weekend AS paid_weekday_weekend,\n",
    "      pd.month_name AS paid_month_name,\n",
    "      pd.calendar_quarter AS paid_calendar_quarter,\n",
    "      pd.calendar_year AS paid_calendar_year,\n",
    "      o.shipped_date_key,\n",
    "      sd.day_name_of_week AS shipped_day_name_of_week,\n",
    "      sd.day_of_month AS shipped_day_of_month,\n",
    "      sd.weekday_weekend AS shipped_weekday_weekend,\n",
    "      sd.month_name AS shipped_month_name,\n",
    "      sd.calendar_quarter AS shipped_calendar_quarter,\n",
    "      sd.calendar_year AS shipped_calendar_year,\n",
    "      o.quantity,\n",
    "      o.unit_price,\n",
    "      o.discount,\n",
    "      o.shipping_fee,\n",
    "      o.taxes,\n",
    "      o.tax_rate,\n",
    "      o.payment_type,\n",
    "      o.order_status,\n",
    "      o.order_details_status\n",
    "  FROM orders_silver_tempview AS o\n",
    "  INNER JOIN northwind_dlh.dim_employee AS e\n",
    "  ON e.employee_key = o.employee_key\n",
    "  INNER JOIN northwind_dlh.dim_customer AS c\n",
    "  ON c.customer_key = o.customer_key\n",
    "  INNER JOIN northwind_dlh.dim_product AS p\n",
    "  ON p.product_key = o.product_key\n",
    "  INNER JOIN northwind_dlh.dim_shipper AS s\n",
    "  ON s.shipper_key = o.shipper_key\n",
    "  LEFT OUTER JOIN northwind_dlh.dim_date AS od\n",
    "  ON od.date_key = o.order_date_key\n",
    "  LEFT OUTER JOIN northwind_dlh.dim_date AS pd\n",
    "  ON pd.date_key = o.paid_date_key\n",
    "  LEFT OUTER JOIN northwind_dlh.dim_date AS sd\n",
    "  ON sd.date_key = o.shipped_date_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "240d8a94-0801-4e89-966d-a08f843efb6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.table(\"fact_orders_silver_tempview\")\n",
    "      .writeStream\n",
    "      .format(\"delta\")\n",
    "      .option(\"checkpointLocation\", f\"{orders_output_silver}/_checkpoint\")\n",
    "      .outputMode(\"append\")\n",
    "      .table(\"fact_orders_silver\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0405389-1528-48ba-96cc-90a9c67f1c34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM fact_orders_silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f89680de-3ae9-4a27-9c0c-a8e4eef7667a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED northwind_dlh.fact_orders_silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6804eb9a-7c90-4c3c-a26e-29c9b91f1b1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 6.3. Gold Table: Perform Aggregations\n",
    "Create a new Gold table using the CTAS approach. The table should include the number of products sold per customer each Month, along with the Customers' ID, First & Last Name, and the Month in which the order was placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "255621fc-ae98-4723-917c-898eb3c0bf96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE northwind_dlh.fact_monthly_orders_by_customer_gold AS (\n",
    "  SELECT customer_key AS CustomerID\n",
    "    , customer_last_name AS LastName\n",
    "    , customer_first_name AS FirstName\n",
    "    , order_month_name AS OrderMonth\n",
    "    , COUNT(product_key) AS ProductCount\n",
    "  FROM northwind_dlh.fact_orders_silver\n",
    "  GROUP BY CustomerID, LastName, FirstName, OrderMonth\n",
    "  ORDER BY ProductCount DESC);\n",
    "\n",
    "SELECT * FROM northwind_dlh.fact_monthly_orders_by_customer_gold;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff0d151-e0d9-497d-b624-3c84dfbdb714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE northwind_dlh.fact_product_orders_by_customer_gold AS (\n",
    "  SELECT pc.CustomerID\n",
    "    , os.customer_last_name AS CustomerName\n",
    "    , os.product_key AS ProductNumber\n",
    "    , pc.ProductCount\n",
    "  FROM northwind_dlh.fact_orders_silver AS os\n",
    "  INNER JOIN (\n",
    "    SELECT customer_key AS CustomerID\n",
    "    , COUNT(product_key) AS ProductCount\n",
    "    FROM northwind_dlh.fact_orders_silver\n",
    "    GROUP BY customer_key\n",
    "  ) AS pc\n",
    "  ON pc.CustomerID = os.customer_key\n",
    "  ORDER BY ProductCount DESC);\n",
    "\n",
    "SELECT * FROM northwind_dlh.fact_product_orders_by_customer_gold;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfc14913-6bc4-4460-8c2b-f41a9b72e4c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 7.0. Use AutoLoader to Process Streaming (Hot Path) Purchase Orders Fact Data \n",
    "##### 7.1. Bronze Table: Process 'Raw' JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e164c5f-4125-4d39-8e34-a9777ecc74a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use spark.readStream and the AutoLoader to read in the JSON files in the \"purchase_orders_stream_dir\"\n",
    "# directory and then create a TempView named \"purchase_orders_raw_tempview\".\n",
    "# Be sure to set the \"cloudFiles.schemaLocation\" Option using the \"purchase_orders_output_bronze\" directory\n",
    "(spark.readStream\n",
    " .format(\"cloudFiles\")\n",
    " .option(\"cloudFiles.format\", \"json\")\n",
    " .option(\"cloudFiles.schemaLocation\", purchase_orders_output_bronze)\n",
    " .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    " .option(\"multiLine\", \"true\")\n",
    " .load(purchase_orders_stream_dir)\n",
    " .createOrReplaceTempView(\"purchase_orders_raw_tempview\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38d11512-c082-420c-ba5f-9506bd6e048b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "/* Add Metadata for Traceability */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "369910e4-19f7-4568-a23c-851436e6dd04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM purchase_orders_bronze_tempview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "591b24a4-cd8f-4090-9dda-40e523e5285d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.table(\"purchase_orders_bronze_tempview\")\n",
    "      .writeStream\n",
    "      .format(\"delta\")\n",
    "      .option(\"checkpointLocation\", f\"{purchase_orders_output_bronze}/_checkpoint\")\n",
    "      .outputMode(\"append\")\n",
    "      .table(\"fact_purchase_orders_bronze\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef4d8b4e-4002-481b-b805-89d43b3299ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 7.2. Silver Table: Include Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64997d8b-f699-41f7-9dd8-020706e1bc98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream\n",
    "  .table(\"fact_purchase_orders_bronze\")\n",
    "  .createOrReplaceTempView(\"purchase_orders_silver_tempview\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5fb1d96-374e-49e2-9aaa-d2eb6b201e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM purchase_orders_silver_tempview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6cf19c8-d178-461d-b10c-16ca87a2a882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED purchase_orders_silver_tempview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d20e510-7ba2-420f-bac9-47bf2ac9a7fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create a new Temporary View named \"purchase_orders_silver_tempview\" by selecting data from\n",
    "-- \"purchase_orders_silver_tempview\" and joining it to the Supplier, Product, Employee and Date dimension tables.\n",
    "-- Remember that the Date dimension can serve as a \"Role Playing\" dimension by being Joined upon multiple times.\n",
    "CREATE OR REPLACE TEMPORARY VIEW fact_purchase_orders_silver_tempview AS (\n",
    "  SELECT po.purchase_order_key,\n",
    "      po.employee_key,\n",
    "      e.last_name AS employee_last_name,\n",
    "      e.first_name AS employee_first_name,\n",
    "      po.supplier_key,\n",
    "      s.company AS supplier_company,\n",
    "      po.product_key,\n",
    "      p.product_name,\n",
    "      p.standard_cost,\n",
    "      p.list_price,\n",
    "      po.submitted_date_key,\n",
    "      sd.month_name AS submitted_month,\n",
    "      sd.calendar_quarter AS submitted_quarter,\n",
    "      po.creation_date_key,\n",
    "      cd.month_name AS creation_month,\n",
    "      cd.calendar_quarter AS creation_quarter,\n",
    "      po.status_key,\n",
    "      po.expected_date_key,\n",
    "      po.shipping_fee,\n",
    "      po.taxes,\n",
    "      po.payment_date_key,\n",
    "      po.payment_amount,\n",
    "      po.payment_method,\n",
    "      po.notes\n",
    "  FROM purchase_orders_silver_tempview AS po\n",
    "  INNER JOIN northwind_dlh.dim_employee AS e\n",
    "  ON e.employee_key = po.employee_key\n",
    "  INNER JOIN northwind_dlh.dim_supplier AS s \n",
    "  ON s.supplier_key = po.supplier_key\n",
    "  INNER JOIN northwind_dlh.dim_product AS p\n",
    "  ON p.product_key = po.product_key\n",
    "  LEFT OUTER JOIN northwind_dlh.dim_date AS sd\n",
    "  ON sd.date_key = po.submitted_date_key\n",
    "  LEFT OUTER JOIN northwind_dlh.dim_date AS cd\n",
    "  ON cd.date_key = po.creation_date_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95519b99-c294-4b0b-9c24-f36383d8602a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.table(\"fact_purchase_orders_silver_tempview\")\n",
    "      .writeStream\n",
    "      .format(\"delta\")\n",
    "      .option(\"checkpointLocation\", f\"{purchase_orders_output_silver}/_checkpoint\")\n",
    "      .outputMode(\"append\")\n",
    "      .table(\"fact_purchase_orders_silver\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f104d194-8005-4468-b57a-babc6cf11014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM fact_purchase_orders_silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd525449-f99f-4fb2-b764-62efcf42ed92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED fact_purchase_orders_silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d707a46f-7cf1-4a89-814b-2f473afd7cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 7.3. Gold Table: Perform Aggregations\n",
    "Create a new Gold table using the CTAS approach. The table should include the total amount (total list price) of the purchase orders placed per Supplier for each Product. Include the Suppliers' Company Name, and the Product Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4589465a-fdcc-4cd1-b08f-b99a22319f4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Author a query that returns the Total List Price grouped by Supplier and Product and sorted by Total List Price descending.\n",
    "CREATE OR REPLACE TABLE northwind_dlh.fact_inventory_transaction_summary_gold AS (\n",
    "  SELECT created_quarter AS Quarter,\n",
    "         transaction_type AS TransactionType,\n",
    "         product_name AS Product,\n",
    "         SUM(quantity) AS TotalQuantity\n",
    "  FROM northwind_dlh.fact_inventory_transactions_silver\n",
    "  GROUP BY Quarter, TransactionType, Product\n",
    "  ORDER BY TotalQuantity DESC\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d871224-85b8-47f4-b723-cf0b88840464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 8.0. Use AutoLoader to Process Streaming (Hot Path) Inventory Transactions Fact Data \n",
    "##### 8.1. Bronze Table: Process 'Raw' JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "045ebc74-f39c-4f26-af1f-e73c91cdf944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use spark.readStream and the AutoLoader to read in the JSON files in the \"inventory_trans_stream_dir\"\n",
    "# directory and then create a TempView named \"inventory_transactions_raw_tempview\".\n",
    "# Be sure to set the \"cloudFiles.schemaLocation\" Option using the \"inventory_trans_output_bronze\" directory\n",
    "(spark.readStream\n",
    " .format(\"cloudFiles\")\n",
    " .option(\"cloudFiles.format\", \"json\")\n",
    " .option(\"cloudFiles.schemaLocation\", inventory_trans_output_bronze)\n",
    " .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    " .option(\"multiLine\", \"true\")\n",
    " .load(inventory_trans_stream_dir)\n",
    " .createOrReplaceTempView(\"inventory_transactions_raw_tempview\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f95bdd-ef5e-4a01-b59c-3d60ee9b2677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "/* Add Metadata for Traceability */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05db6c40-f3a6-4149-9431-c88ce472f65a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM inventory_transactions_raw_tempview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecdd97a8-3a53-4996-af65-ab1765da1589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.table(\"inventory_transactions_bronze_tempview\")\n",
    "     .writeStream\n",
    "     .format(\"delta\")\n",
    "     .option(\"checkpointLocation\", f\"{inventory_trans_output_bronze}/_checkpoint\")\n",
    "     .outputMode(\"append\")\n",
    "     .table(\"fact_inventory_transactions_bronze\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc67185-6093-4216-8a3f-85a1b168cbc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 8.2. Silver Table: Include Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e45489-6a37-4250-85bf-c8739f32fc0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream\n",
    "  .table(\"fact_inventory_transactions_bronze\")\n",
    "  .createOrReplaceTempView(\"inventory_transactions_silver_tempview\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3c72d37-3d06-4ce5-bf51-8da71c33781b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM inventory_transactions_silver_tempview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ded4ac4-c6de-4c9a-8dcd-b0b7568c2db5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED inventory_transactions_silver_tempview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "616be98c-9e78-4f25-ae89-957693d79662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create a new Temporary View named \"fact_inventory_transactions_silver_tempview\" by selecting data from\n",
    "-- \"inventory_transactions_silver_tempview\" and joining it to the Product and Data dimension tables.\n",
    "-- Remember that the Date dimension can serve as a \"Role Playing\" dimension by being Joined upon multiple times.\n",
    "CREATE OR REPLACE TEMPORARY VIEW fact_inventory_transactions_silver_tempview AS (\n",
    "  SELECT \n",
    "      it.inventory_transaction_key,\n",
    "      it.transaction_type,\n",
    "      it.transaction_created_date_key,\n",
    "      it.transaction_modified_date_key,\n",
    "      it.product_key,\n",
    "      it.quantity,\n",
    "      it.comments,\n",
    "      \n",
    "      -- Product dimension attributes\n",
    "      p.product_code,\n",
    "      p.product_name,\n",
    "      p.description,\n",
    "      p.supplier_company,\n",
    "      p.standard_cost,\n",
    "      p.list_price,\n",
    "      p.reorder_level,\n",
    "      p.target_level,\n",
    "      p.quantity_per_unit,\n",
    "      p.discontinued,\n",
    "      p.minimum_reorder_quantity,\n",
    "      p.category,\n",
    "      \n",
    "      -- Created Date dimension attributes (role-playing)\n",
    "      cd.date_key as created_date_key,\n",
    "      cd.full_date_alternate_key as created_full_date,\n",
    "      cd.day_number_of_week as created_day_number_of_week,\n",
    "      cd.day_name_of_week as created_day_name_of_week,\n",
    "      cd.day_number_of_month as created_day_of_month,\n",
    "      cd.day_number_of_year as created_day_of_year,\n",
    "      cd.weekday_weekend as created_weekday_weekend,\n",
    "      cd.week_number_of_year as created_week_of_year,\n",
    "      cd.month_name as created_month_name,\n",
    "      cd.month_number_of_year as created_month_number,\n",
    "      cd.calendar_quarter as created_quarter,\n",
    "      cd.calendar_year as created_year,\n",
    "      cd.calendar_semester as created_semester,\n",
    "      \n",
    "      -- Modified Date dimension attributes (role-playing)\n",
    "      md.date_key as modified_date_key,\n",
    "      md.full_date_alternate_key as modified_full_date,\n",
    "      md.day_number_of_week as modified_day_number_of_week,\n",
    "      md.day_name_of_week as modified_day_name_of_week,\n",
    "      md.day_number_of_month as modified_day_of_month,\n",
    "      md.day_number_of_year as modified_day_of_year,\n",
    "      md.weekday_weekend as modified_weekday_weekend,\n",
    "      md.week_number_of_year as modified_week_of_year,\n",
    "      md.month_name as modified_month_name,\n",
    "      md.month_number_of_year as modified_month_number,\n",
    "      md.calendar_quarter as modified_quarter,\n",
    "      md.calendar_year as modified_year,\n",
    "      md.calendar_semester as modified_semester\n",
    "      \n",
    "  FROM inventory_transactions_silver_tempview it\n",
    "  \n",
    "  INNER JOIN northwind_dlh.dim_product p\n",
    "  ON it.product_key = p.product_key\n",
    "  \n",
    "  LEFT OUTER JOIN northwind_dlh.dim_date cd\n",
    "  ON it.transaction_created_date_key = cd.date_key\n",
    "  \n",
    "  LEFT OUTER JOIN northwind_dlh.dim_date md\n",
    "  ON it.transaction_modified_date_key = md.date_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b555412e-e498-4585-9e7b-e52c194e14a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.table(\"fact_inventory_transactions_silver_tempview\")\n",
    "      .writeStream\n",
    "      .format(\"delta\")\n",
    "      .option(\"checkpointLocation\", f\"{inventory_trans_output_silver}/_checkpoint\")\n",
    "      .outputMode(\"append\")\n",
    "      .table(\"fact_inventory_transactions_silver\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7db4b79-69f5-43f3-bdce-c4fb4c8b2176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM fact_inventory_transactions_silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a094cc61-d1d5-4537-a389-fa8e126a0c60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED fact_inventory_transactions_silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf279dad-12f0-4a55-a821-ecc61d653d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 8.3. Gold Table: Perform Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4d66c1d-4712-4cc1-bf4d-0d297cd6a1fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Author a query that returns the Total Quantity grouped by the Quarter Created, Inventory Transaction Type, and Product\n",
    "-- Sort by the Total Quantity Descending\n",
    "CREATE OR REPLACE TABLE northwind_dlh.fact_inventory_transaction_summary_gold AS (\n",
    "  SELECT created_quarter AS Quarter,\n",
    "         transaction_type AS TransactionType,\n",
    "         product_name AS Product,\n",
    "         SUM(quantity) AS TotalQuantity\n",
    "  FROM northwind_dlh.fact_inventory_transactions_silver\n",
    "  GROUP BY Quarter, TransactionType, Product\n",
    "  ORDER BY TotalQuantity DESC\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "143f665e-eda3-4fbf-8f72-9aab08ae302b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 9.0. Clean up the File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d6447df-9b1b-4ca6-aab4-9651998583d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs rm -r /FileStore/lab_data/"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 997251033567230,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "6.0-Lab-DataLakehouse-Structured-Streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
